---
title: "Alcohol and study"
author: "Pereira y Centocchi"
date: "2024-08-24"
output: pdf_document
---



```{r setup, include=FALSE}
# Configuraciones iniciales

knitr::opts_chunk$set(echo = TRUE)
library(rpart)
library(dplyr)
```

```{r}
data_original <- read.csv("C:/Users/fpereira/Downloads/smoking_driking_dataset_Ver01.csv")

set.seed(123) # Fijar semilla para reproducibilidad
data <- sample_n(data_original, 50000)


```

```{r}
# Mostrar las primeras filas del conjunto de datos
head(data)
```


## Recodificación de Columnas Numéricas a Categóricas

```{r recode-columns, message=FALSE, warning=FALSE}
library(dplyr)

# Recodificar la columna hear_right
data <- data %>%
  mutate(hear_right = recode(hear_right,
                             `1` = "Normal",
                             `2` = "Abnormal"))

# Recodificar la columna hear_left
data <- data %>%
  mutate(hear_left = recode(hear_left,
                            `1` = "Normal",
                            `2` = "Abnormal"))

# Recodificar la columna SMK_stat_type_cd
data <- data %>%
  mutate(SMK_stat_type_cd = recode(SMK_stat_type_cd,
                                   `1` = "No Fumador",
                                   `2` = "Ex Fumador",
                                   `3` = "Fumador Actual"))
# Recodificar la columna DRK_YN
data <- data %>%
  mutate(DRK_YN = recode(DRK_YN,
                            `1` = "Y",
                            `2` = "N"))
# Recodificar la columna urine_protein
data <- data %>%
  mutate(urine_protein = recode(urine_protein,
                                `1` = "-",
                                `2` = "+/-",
                                `3` = "+1",
                                `4` = "+2",
                                `5` = "+3",
                                `6` = "+4"))

# Mostrar las primeras filas del dataset recodificado
head(data)



```

```{r}
# Resumen estadístico de los datos
summary(data)
```



## Distribución de las Columnas Numéricas

## Graficar la Distribución de Variables Numéricas y No Numéricas

```{r distribution-plot-all, message=FALSE, warning=FALSE}
library(ggplot2)

# Seleccionar las columnas numéricas y no numéricas
numeric_cols <- data[sapply(data, is.numeric)]
non_numeric_cols <- data[sapply(data, Negate(is.numeric))]

# Graficar la distribución de cada columna numérica
numeric_plots <- lapply(names(numeric_cols), function(col) {
  ggplot(data, aes_string(x = col)) +
    geom_histogram(fill = "skyblue", color = "black", bins = 50) +
    labs(title = paste("Distribución de", col), x = col, y = "Frecuencia") +
    theme_minimal()
})

# Graficar la distribución de cada columna no numérica
non_numeric_plots <- lapply(names(non_numeric_cols), function(col) {
  ggplot(data, aes_string(x = col)) +
    geom_bar(fill = "skyblue", color = "black") +
    labs(title = paste("Distribución de", col), x = col, y = "Frecuencia") +
    theme_minimal()
})

# Combinar todas las gráficas en una lista
plot_list <- c(numeric_plots, non_numeric_plots)

# Mostrar las gráficas
plot_list

```


```{r correlation-matrix-groups, message=FALSE, warning=FALSE}
library(ggcorrplot)
# Dividir las columnas en grupos de 5 (puedes ajustar el tamaño del grupo)
group_size <- 9
col_groups <- split(names(numeric_cols), ceiling(seq_along(names(numeric_cols))/group_size))

# Graficar la matriz de correlación para cada grupo
for (cols in col_groups) {
  corr_matrix_group <- cor(numeric_cols[cols], use = "complete.obs")
  
  print(ggcorrplot(corr_matrix_group, 
                   method = "circle", 
                   type = "lower", 
                   lab = TRUE, 
                   lab_size = 3, 
                   colors = c("red", "white", "blue"), 
                   title = paste("Matriz de Correlación - Grupo:", paste(cols, collapse = ", ")), 
                   ggtheme = theme_minimal()))
}
```



```{r correlation-matrix-filtered, message=FALSE, warning=FALSE}
library(ggcorrplot)

# Calcular la matriz de correlación
corr_matrix <- cor(numeric_cols, use = "complete.obs")

# Filtrar correlaciones significativas (mayores en valor absoluto a 0.5)
corr_matrix_filtered <- corr_matrix
corr_matrix_filtered[abs(corr_matrix_filtered) < 0.5 ] <- NA

# Graficar la matriz de correlación filtrada
ggcorrplot(corr_matrix_filtered, 
           method = "circle", 
           type = "lower", 
           lab = TRUE,
           lab_size = 3, 
           colors = c("red", "white", "blue"), 
           title = "Matriz de Correlación Filtrada", 
           ggtheme = theme_minimal())

```

## Correlación con la Columna `DRK_YN`

## Cálculo de Correlación y Reversión de la Conversión

```{r correlation-specific-column-plot-revert, message=FALSE, warning=FALSE}
library(dplyr)
library(ggplot2)

# Guardar el estado original de DRK_YN
original_DRK_YN <- data$DRK_YN

# Verificar si DRK_YN es numérica; si no, convertirla
if (!is.numeric(data$DRK_YN)) {
  data$DRK_YN <- as.numeric(as.factor(data$DRK_YN))
}

# Seleccionar solo columnas numéricas
numeric_cols <- data[sapply(data, is.numeric)]

# Calcular las correlaciones de DRK_YN con todas las demás columnas
correlations <- cor(numeric_cols, use = "complete.obs")["DRK_YN", ]

# Ordenar las correlaciones en orden decreciente (opcional)
correlations <- sort(correlations, decreasing = TRUE)

# Convertir las correlaciones en un data frame para graficar
correlation_df <- data.frame(
  variable = names(correlations),
  correlation = correlations
)

# Graficar las correlaciones
ggplot(correlation_df, aes(x = reorder(variable, correlation), y = correlation)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  coord_flip() +
  labs(title = "Correlación de Variables con DRK_YN", x = "Variables", y = "Correlación") +
  theme_minimal()

# Mostrar las correlaciones
print(correlations)

# Revertir DRK_YN a su estado original
data$DRK_YN <- original_DRK_YN

```

```{r}
# Mostrar las primeras filas del conjunto de datos
head(data)
```

## División del Conjunto de Datos en Entrenamiento, Validación y Testeo

``` {r data-split, message=FALSE, warning=FALSE}
# Fijar la semilla para asegurar la replicabilidad
set.seed(123)

# Número total de observaciones
n <- nrow(data)

# Crear un vector de índices mezclados
indices <- sample(1:n)

# Definir los tamaños para cada partición
train_size <- floor(0.70 * n)
validation_size <- floor(0.15 * n)
test_size <- n - train_size - validation_size

# Crear los índices para cada partición
train_indices <- indices[1:train_size]
validation_indices <- indices[(train_size + 1):(train_size + validation_size)]
test_indices <- indices[(train_size + validation_size + 1):n]

# Dividir el conjunto de datos en las tres particiones
train_data <- data[train_indices, ]
validation_data <- data[validation_indices, ]
test_data <- data[test_indices, ]

# Mostrar el tamaño de cada conjunto
cat("Tamaño del conjunto de entrenamiento:", nrow(train_data), "\n")
cat("Tamaño del conjunto de validación:", nrow(validation_data), "\n")
cat("Tamaño del conjunto de testeo:", nrow(test_data), "\n")
``` 
``` {r entrenamiento default  message=FALSE, warning=FALSE}
#install.packages("rpart.plot")
library(rpart.plot)
arbol_modelo <- rpart(DRK_YN ~ ., data = train_data, method = "class")

# Visualizar el árbol
rpart.plot(arbol_modelo)
``` 


``` {r error-metrics, message=FALSE, warning=FALSE}
# Instalar paquetes necesarios
#install.packages("caret")
#install.packages("e1071")
#install.packages("pROC")

# Cargar librerías
library(caret)
library(pROC)

# Realizar predicciones en el conjunto de validación
predicciones <- predict(arbol_modelo, newdata = validation_data, type = "class")

# Convertir validation_data$DRK_YN a factor y eliminar niveles no usados
validation_data$DRK_YN <- as.factor(validation_data$DRK_YN)
validation_data$DRK_YN <- droplevels(validation_data$DRK_YN)

# Convertir predicciones a factor con los mismos niveles que validation_data$DRK_YN
niveles <- levels(validation_data$DRK_YN)
predicciones <- factor(predicciones, levels = niveles)

# Crear la matriz de confusión
confusion_matrix <- confusionMatrix(predicciones, validation_data$DRK_YN)
print(confusion_matrix)

# Obtener la precisión
accuracy <- confusion_matrix$overall["Accuracy"]
print(accuracy)

# Calcular la precisión y el recall
precision <- confusion_matrix$byClass["Precision"]
recall <- confusion_matrix$byClass["Recall"]
print(precision)
print(recall)

# Calcular el F1-score
f1_score <- 2 * (precision * recall) / (precision + recall)
print("f1_score")
print(f1_score)

# Calcular el AUC-ROC
roc_curve <- roc(as.numeric(validation_data$DRK_YN), as.numeric(predicciones))
auc_roc <- auc(roc_curve)

# Mostrar el AUC-ROC
print(auc_roc)


```

```{r}
arbol_modelo <- rpart(DRK_YN ~ ., data = train_data, method = "class",
                      cp = 0,           # Parámetro de complejidad
                      xval = 0,
                      minsplit = 100,       # Mínimo número de observaciones en un nodo antes de dividir
                      minbucket = 100,     # Mínimo número de observaciones en un nodo terminal
                      maxdepth = 9)        # Profundidad máxima del árbol

# Calcular el AUC-ROC
predicciones <- predict(arbol_modelo, newdata = validation_data, type = "class")
roc_curve <- roc(as.numeric(validation_data$DRK_YN), as.numeric(predicciones))
auc_roc <- auc(roc_curve)

# Mostrar el AUC-ROC
print(auc_roc)

```

```{r}
# Cargar las librerías necesarias
library(rpart)
library(pROC)

# Inicializar un data frame para guardar los resultados
resultados <- data.frame(minsplit = integer(),
                         minbucket = integer(),
                         maxdepth = integer(),
                         auc_roc = numeric(),
                         stringsAsFactors = FALSE)

# Definir los rangos de los hiperparámetros
minsplit_values <- seq(0, 100, by = 10)
minbucket_values <- seq(0, 150, by = 10)
maxdepth_values <- seq(1, 15, by = 1)

# Triple for para recorrer todas las combinaciones de hiperparámetros
for (minsplit in minsplit_values) {
  for (minbucket in minbucket_values) {
    for (maxdepth in maxdepth_values) {
      
      # Entrenar el modelo con la combinación actual de hiperparámetros
      arbol_modelo <- rpart(DRK_YN ~ ., data = train_data, method = "class",
                            cp = 0,           # Parámetro de complejidad
                            xval = 0,
                            minsplit = minsplit,       # Mínimo número de observaciones en un nodo antes de dividir
                            minbucket = minbucket,     # Mínimo número de observaciones en un nodo terminal
                            maxdepth = maxdepth)        # Profundidad máxima del árbol
      
      # Calcular el AUC-ROC
      predicciones <- predict(arbol_modelo, newdata = validation_data, type = "class")
      roc_curve <- roc(as.numeric(validation_data$DRK_YN), as.numeric(predicciones), quiet = TRUE)
      auc_roc <- auc(roc_curve)
      
      # Guardar los resultados en el data frame
      resultados <- rbind(resultados, data.frame(minsplit = minsplit, 
                                                 minbucket = minbucket, 
                                                 maxdepth = maxdepth, 
                                                 auc_roc = auc_roc))
    }
  }
}

# Mostrar los resultados
print(resultados)

```

```{r}
# Encontrar el índice del máximo AUC-ROC
max_index <- which.max(resultados$auc_roc)

# Extraer los valores correspondientes a ese índice
max_auc_roc <- resultados$auc_roc[max_index]
best_params <- resultados[max_index, ]

# Imprimir el AUC-ROC máximo y los hiperparámetros correspondientes
cat("El máximo AUC-ROC es:", max_auc_roc, "\n")
cat("Logrado con minsplit =", best_params$minsplit, 
    ", minbucket =", best_params$minbucket, 
    ", maxdepth =", best_params$maxdepth, "\n")
```

```{r}


# Encontrar el mejor AUC-ROC para cada valor de minsplit
best_by_minsplit <- aggregate(auc_roc ~ minsplit, data = resultados, max)

# Gráfico para minsplit
ggplot(best_by_minsplit, aes(x = minsplit, y = auc_roc)) +
  geom_line() +
  geom_point() +
  labs(title = "Mejor AUC-ROC por minsplit",
       x = "minsplit",
       y = "AUC-ROC") +
  theme_minimal()

# Encontrar el mejor AUC-ROC para cada valor de minbucket
best_by_minbucket <- aggregate(auc_roc ~ minbucket, data = resultados, max)

# Gráfico para minbucket
ggplot(best_by_minbucket, aes(x = minbucket, y = auc_roc)) +
  geom_line() +
  geom_point() +
  labs(title = "Mejor AUC-ROC por minbucket",
       x = "minbucket",
       y = "AUC-ROC") +
  theme_minimal()

# Encontrar el mejor AUC-ROC para cada valor de maxdepth
best_by_maxdepth <- aggregate(auc_roc ~ maxdepth, data = resultados, max)

# Gráfico para maxdepth
ggplot(best_by_maxdepth, aes(x = maxdepth, y = auc_roc)) +
  geom_line() +
  geom_point() +
  labs(title = "Mejor AUC-ROC por maxdepth",
       x = "maxdepth",
       y = "AUC-ROC") +
  theme_minimal()

```

